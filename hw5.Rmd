---
title: "Homework 6"
author: "Alyssa Vanderbeek (amv2187)"
date: "3 December 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(faraway)
library(HH)
library(leaps)
library(caret)
```

```{r}
states = state.x77 %>% # load data from faraway package
  as.data.frame() %>%
  janitor::clean_names()
```

**1. Explore the dataset and generate appropriate descriptive statistics and relevant graphs for all variables of interest (continuous and categorical) – no test required. Be selective! Even if you create 20 plots, you don’t want to show them all.**

```{r}

# table of summary stats
states %>%
  skimr::skim_to_list() %>%
  as.data.frame %>%
  dplyr::select(1, 2, 5:11) %>%
  `colnames<-`(c(' ', 'NA', 'Mean', 'Std. Dev.', 'Min', '1st Q', 'Median', '3rd Q', 'Max')) %>%
  knitr::kable()

# scatterplot to assess correlation between vars
states %>%
  pairs

# correlation matrix to evaluate what is seen in scatterplots
states %>%
  cor
```

It looks like murder is correlated both with life expectancy and illiteracy, suggesting that it is a potential confounder. Specifically, murder is positively associated with illiteracy (higher murder rate = higher illiteracy rate) and negatively associated with life expectancy (higher murder rate = lower life expectancy).

After examining the distribution of each vriable in the dataset, I chose to perform a log transformation on the estimates for area size, illiteracy rate, and population size, which were all skewed. 

```{r}
states_analysis = states %>%
  mutate(log_area = log(area),
         log_illiteracy = log(illiteracy),
         log_pop = log(population)) %>%
  dplyr::select(-area, -population, -illiteracy)

par(mfrow = c(2, 3))
hist(states$illiteracy)
hist(states$population)
hist(states$area)
hist(states_analysis$log_illiteracy)
hist(states_analysis$log_pop)
hist(states_analysis$log_area)

```


**2. Use automatic procedures to find a ‘best subset’ of the full model. Present the results and comment on the following**

```{r}
## backwards elimination
# summary(lm(life_exp ~ ., data = states_analysis))
# summary(lm(life_exp ~ murder + hs_grad + frost + log_area + log_illiteracy + log_pop, data = states_analysis))
# summary(lm(life_exp ~ murder + hs_grad + frost + log_illiteracy + log_pop, data = states_analysis))

b.fit = lm(life_exp ~ murder + hs_grad + frost + log_pop, data = states_analysis)
summary(b.fit)


## forwards process
# summary(lm(life_exp ~ murder, data = states_analysis))
# summary(lm(life_exp ~ hs_grad, data = states_analysis))
# summary(lm(life_exp ~ frost, data = states_analysis))
# summary(lm(life_exp ~ log_area, data = states_analysis))
# summary(lm(life_exp ~ log_illiteracy, data = states_analysis))
# summary(lm(life_exp ~ log_pop, data = states_analysis))
# 
# # murder has lowest p-val. Start adding secondary vars
# summary(lm(life_exp ~ murder + hs_grad, data = states_analysis))
# summary(lm(life_exp ~ murder + frost, data = states_analysis))
# summary(lm(life_exp ~ murder + log_area, data = states_analysis))
# summary(lm(life_exp ~ murder + log_illiteracy, data = states_analysis))
# summary(lm(life_exp ~ murder + log_pop, data = states_analysis))
# 
# # murder + hs_grad
# summary(lm(life_exp ~ murder + hs_grad + frost, data = states_analysis))
# summary(lm(life_exp ~ murder + hs_grad + log_area, data = states_analysis))
# summary(lm(life_exp ~ murder + hs_grad + log_illiteracy, data = states_analysis))
# summary(lm(life_exp ~ murder + hs_grad + log_pop, data = states_analysis))
# 
# # murder + hs_grad + log_pop
# summary(lm(life_exp ~ murder + hs_grad + log_pop + frost, data = states_analysis))
# summary(lm(life_exp ~ murder + hs_grad + log_pop + log_area, data = states_analysis))
# summary(lm(life_exp ~ murder + hs_grad + log_pop + log_illiteracy, data = states_analysis))
# 
# # murder + hs_grad + log_pop + frost
# summary(lm(life_exp ~ murder + hs_grad + log_pop + frost + log_area, data = states_analysis))
# summary(lm(life_exp ~ murder + hs_grad + log_pop + frost + log_illiteracy, data = states_analysis))

f.fit = lm(life_exp ~ murder + hs_grad + log_pop + frost, data = states_analysis)
summary(f.fit)


## Stepwise
step.fit = step(lm(life_exp ~ ., data = states_analysis))

```

All automatic processes conclude the same model, using percent increase in population size (log(population)), rate of high school graduation (hs_grad), murder rate per 100,000 (murder), and average number of days annually with temperatures below freezing (frost) as predictors of life expectancy. 

'Frost' is the least significant predictor, with a p-value of 0.043. No variables were seen to be a "close call" at the 5% significance level.

There is a correlation between illiteracy (with and without log transformation) and high school graduation rate (`r cor(states$illiteracy, states$hs_grad)`), but my model includes only high school graduation rate as a predictor.

**3. Use criterion-based procedures studied in class to guide your selection of the ‘best subset’. Summarize your results (tabular or graphical)**

```{r}
best <- function(model, ...) 
{
  subsets <- regsubsets(formula(model), model.frame(model), ...)
  subsets <- with(summary(subsets),
                  cbind(p = as.numeric(rownames(which)), which, rss, rsq, adjr2, cp, bic))
  
  return(subsets)
} 

best(lm(life_exp ~ ., data = states_analysis)) %>%
  knitr::kable(., 'latex', caption = 'Criterion-based model building') %>%
  kableExtra::kable_styling(latex_options = c("hold_position")) %>%
  kableExtra::landscape()

# leaps::leaps(x = states_analysis[, c(1, 3:8)], y = states_analysis$life_exp, nbest = 2, method = "Cp")

# leaps::leaps(x = states_analysis[, c(1, 3:8)], y = states_analysis$life_exp, nbest = 2, method = "adjr2")

# Summary of models for each size (one model per size)
b = leaps::regsubsets(life_exp ~ ., data = states_analysis)
rs = summary(b)

# Plots of Cp and Adj-R2 as functions of parameters
par(mar = c(4, 4, 1, 1))
par(mfrow = c(1, 2))

plot(1:7, rs$cp, xlab = "No of parameters", ylab = "Cp Statistic")
abline(0, 1)
plot(1:7, rs$adjr2, xlab = "No of parameters", ylab = "Adj R2")

```

According to the Cp statistics and Adjusted R^2, the ideal number of parameters is 4; as seen in the table above, those parameters are murder, hs_grad, frost, and log_pop.


**4. Compare the two ‘subsets’ from parts 2 and 3 and recommend a ‘final’ model. Using this ‘final’ model do the following.**
**a) Identify any leverage and/or influential points and take appropriate measures.**
**b) Check the model assumptions. **

All analyses above recommend the same model using percent increase in population size (log(population)), rate of high school graduation (hs_grad), murder rate per 100,000 (murder), and average number of days annually with temperatures below freezing (frost) as predictors of life expectancy.

```{r}
life_exp_fit = b.fit

# rstandard function gives the INTERNALLY studentized residuals 

stu_res = rstandard(life_exp_fit)
outliers_y = stu_res[abs(stu_res) > 2.5]

# Measures of influence:
# Gives DFFITS, Cook's Distance, Hat diagonal elements, and others.

influence.measures(life_exp_fit)

# Look at the Cook's distance lines / influential point output and notice obs 11 as potential Y outlier / influential point

par(mfrow = c(2, 2))
plot(life_exp_fit)

# Examine results with and without observations 5 and 28 that have very high survivals (>2000)
fit_nooutlier = lm(life_exp ~ murder + hs_grad + log_pop + frost, data = states_analysis[-11, ])
summary(fit_nooutlier) # look at the results of the fitted model without the influential point

par(mfrow = c(2, 2))
plot(life_exp_fit)
plot(fit_nooutlier)

```

The 11th entry showed evidence of being an influential outlier (according to Cook's distance and measure of influence). To check to see whether this entry had a significant impact on the model and its assumptions, I compared diagnostics of the model with and without the 11th point. The diagnostic plots above show that, in fact, the model assumptions (1. residuals have mean zero, 2. residuals have equal variance, 3. residuals are independent) are met for both models - with and without the potential influential point. However, we can see that without the point, the 'frost' variable is no longer a significant predictor of life expectancy, with a p-value of 0.53. 


**Using the ‘final’ model chosen in part 4, focus on MSE to test the model predictive ability**
**a) Use a 10-fold cross-validation**

```{r}
# create 10-fold training datasets
data_train <- trainControl(method = "cv", number = 10)

# Fit the model used above
model_caret <- train((life_exp ~ murder + hs_grad + log_pop + frost),
                   data = states_analysis,
                   trControl = data_train,
                   method = 'lm',
                   na.action = na.pass)
  
# Model predictions using 4 parts of the data from training 
model_caret

```

**(b) Experiment a new, but simple bootstrap technique called “residual sampling”.**

```{r}
# Perform a regression model with the original sample; calculate predicted values and residuals.
states_analysis = states_analysis %>%
  modelr::add_predictions(life_exp_fit) %>% # add predicted birthweight
  modelr::add_residuals(life_exp_fit) # residual of observed bwt - predicted bwt
  
# Randomly resample the residuals (with replacement), but leave the X values and predicted values unchanged.

```

