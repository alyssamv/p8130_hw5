---
title: "Homework 6"
author: "Alyssa Vanderbeek (amv2187)"
date: "3 December 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, fig.width = 6, fig.height = 6)

library(tidyverse)
library(faraway)
library(HH)
library(leaps)
library(caret)
```

```{r}
states = state.x77 %>% # load data from faraway package
  as.data.frame() %>%
  janitor::clean_names()
```

**1. Explore the dataset and generate appropriate descriptive statistics and relevant graphs for all variables of interest (continuous and categorical) – no test required. Be selective! Even if you create 20 plots, you don’t want to show them all.**

```{r}

# table of summary stats
states %>%
  skimr::skim_to_list() %>%
  as.data.frame %>%
  dplyr::select(1, 2, 5:11) %>%
  `colnames<-`(c(' ', 'NA', 'Mean', 'Std. Dev.', 'Min', '1st Q', 'Median', '3rd Q', 'Max')) %>%
  knitr::kable(caption = 'Summary statistics')
```


```{r}
# scatterplot to assess correlation between vars
states %>% pairs

# correlation matrix to evaluate what is seen in scatterplots
# states %>%
#   cor
```

It looks like murder is correlated both with life expectancy (`r cor(states$life_exp, states$murder)`) and illiteracy (`r cor(states$murder, states$illiteracy)`), suggesting that it is a potential confounder. Specifically, murder is positively associated with illiteracy (higher murder rate = higher illiteracy rate) and negatively associated with life expectancy (higher murder rate = lower life expectancy).

After examining the distribution of each variable in the dataset, I chose to perform a log transformation on the estimates for area size, illiteracy rate, and population size, which were all skewed. 

```{r}
states_analysis = states %>%
  mutate(log_area = log(area),
         log_illiteracy = log(illiteracy),
         log_pop = log(population)) %>%
  dplyr::select(-area, -population, -illiteracy)

par(mfrow = c(2, 3))
hist(states$illiteracy)
hist(states$population)
hist(states$area)
hist(states_analysis$log_illiteracy)
hist(states_analysis$log_pop)
hist(states_analysis$log_area)

```


**2. Use automatic procedures to find a ‘best subset’ of the full model. Present the results and comment on the following**

Final model using backwards elimination:

```{r}
## backwards elimination
# summary(lm(life_exp ~ ., data = states_analysis))
# summary(lm(life_exp ~ murder + hs_grad + frost + log_area + log_illiteracy + log_pop, data = states_analysis))
# summary(lm(life_exp ~ murder + hs_grad + frost + log_illiteracy + log_pop, data = states_analysis))

b.fit = lm(life_exp ~ murder + hs_grad + frost + log_pop, data = states_analysis)
summary(b.fit)
```


Final model using forwards process:

```{r}
## forwards process
# summary(lm(life_exp ~ murder, data = states_analysis))
# summary(lm(life_exp ~ hs_grad, data = states_analysis))
# summary(lm(life_exp ~ frost, data = states_analysis))
# summary(lm(life_exp ~ log_area, data = states_analysis))
# summary(lm(life_exp ~ log_illiteracy, data = states_analysis))
# summary(lm(life_exp ~ log_pop, data = states_analysis))
# 
# # murder has lowest p-val. Start adding secondary vars
# summary(lm(life_exp ~ murder + hs_grad, data = states_analysis))
# summary(lm(life_exp ~ murder + frost, data = states_analysis))
# summary(lm(life_exp ~ murder + log_area, data = states_analysis))
# summary(lm(life_exp ~ murder + log_illiteracy, data = states_analysis))
# summary(lm(life_exp ~ murder + log_pop, data = states_analysis))
# 
# # murder + hs_grad
# summary(lm(life_exp ~ murder + hs_grad + frost, data = states_analysis))
# summary(lm(life_exp ~ murder + hs_grad + log_area, data = states_analysis))
# summary(lm(life_exp ~ murder + hs_grad + log_illiteracy, data = states_analysis))
# summary(lm(life_exp ~ murder + hs_grad + log_pop, data = states_analysis))
# 
# # murder + hs_grad + log_pop
# summary(lm(life_exp ~ murder + hs_grad + log_pop + frost, data = states_analysis))
# summary(lm(life_exp ~ murder + hs_grad + log_pop + log_area, data = states_analysis))
# summary(lm(life_exp ~ murder + hs_grad + log_pop + log_illiteracy, data = states_analysis))
# 
# # murder + hs_grad + log_pop + frost
# summary(lm(life_exp ~ murder + hs_grad + log_pop + frost + log_area, data = states_analysis))
# summary(lm(life_exp ~ murder + hs_grad + log_pop + frost + log_illiteracy, data = states_analysis))

f.fit = lm(life_exp ~ murder + hs_grad + log_pop + frost, data = states_analysis)
summary(f.fit)
```

Final model using a stepwise process:

```{r}
## Stepwise
step.fit = step(lm(life_exp ~ ., data = states_analysis))

```

All automatic processes conclude the same model, using percent increase in population size (log(population)), rate of high school graduation (hs_grad), murder rate per 100,000 (murder), and average number of days annually with temperatures below freezing (frost) as predictors of life expectancy. 

'Frost' is the least significant predictor, with a p-value of 0.043. However, no variables were seen to be a "close call" at the 5% significance level.

There is an observed correlation between illiteracy (with and without log transformation) and high school graduation rate (`r cor(states$illiteracy, states$hs_grad)`), but my model includes only high school graduation rate as a predictor.

**3. Use criterion-based procedures studied in class to guide your selection of the ‘best subset’. Summarize your results (tabular or graphical)**

```{r, fig.height=3, fig.width=6}
# function to select the 'best' model
best <- function(model, ...) 
{
  subsets <- regsubsets(formula(model), model.frame(model), ...)
  subsets <- with(summary(subsets),
                  cbind(p = as.numeric(rownames(which)), which, rss, rsq, adjr2, cp, bic))
  
  return(subsets)
} 

best(lm(life_exp ~ ., data = states_analysis)) %>%
  knitr::kable(., 'latex', caption = 'Criterion-based model building') %>%
  kableExtra::kable_styling(latex_options = c("hold_position")) %>%
  kableExtra::landscape()

# leaps::leaps(x = states_analysis[, c(1, 3:8)], y = states_analysis$life_exp, nbest = 2, method = "Cp")

# leaps::leaps(x = states_analysis[, c(1, 3:8)], y = states_analysis$life_exp, nbest = 2, method = "adjr2")

# Summary of models for each size (one model per size)
b = leaps::regsubsets(life_exp ~ ., data = states_analysis)
rs = summary(b)

# Plots of Cp and Adj-R2 as functions of parameters
par(mar = c(4, 4, 1, 1))
par(mfrow = c(1, 2))

plot(1:7, rs$cp, xlab = "No of parameters", ylab = "Cp Statistic")
abline(0, 1)
plot(1:7, rs$adjr2, xlab = "No of parameters", ylab = "Adj R2")

```

According to the Cp statistics and Adjusted R^2, the ideal number of parameters is 4; as seen in the table above, those parameters are murder, hs_grad, frost, and log_pop - the same as what was concluded in the automatic process.


**4. Compare the two ‘subsets’ from parts 2 and 3 and recommend a ‘final’ model. Using this ‘final’ model do the following.**
**a) Identify any leverage and/or influential points and take appropriate measures.**
**b) Check the model assumptions. **

All analyses above recommend the same model using percent increase in population size (log(population)), rate of high school graduation (hs_grad), murder rate per 100,000 (murder), and average number of days annually with temperatures below freezing (frost) as predictors of life expectancy.

```{r}
life_exp_fit = b.fit

# rstandard function gives the INTERNALLY studentized residuals 

stu_res = rstandard(life_exp_fit)
outliers_y = stu_res[abs(stu_res) > 2.5]

# Measures of influence:
# Gives DFFITS, Cook's Distance, Hat diagonal elements, and others.

# influence.measures(life_exp_fit)

# Look at the Cook's distance lines / influential point output and notice obs 11 as potential Y outlier / influential point

par(mfrow = c(2, 2))
plot(life_exp_fit)

# Examine results with and without observations 5 and 28 that have very high survivals (>2000)
fit_nooutlier = lm(life_exp ~ murder + hs_grad + log_pop + frost, data = states_analysis[-11, ])
summary(fit_nooutlier) # look at the results of the fitted model without the influential point

plot(fit_nooutlier)

```

The 11th entry showed evidence of being an influential outlier (according to Cook's distance and measure of influence). To check to see whether this entry had a significant impact on the model and its assumptions, I compared diagnostics of the model with and without the 11th point. The diagnostic plots above show that, in fact, the model assumptions (1. residuals have mean zero, 2. residuals have equal variance, 3. residuals are independent) are met for both models - with and without the potential influential point. However, we can see that without the point, the 'frost' variable is no longer a significant predictor of life expectancy, with a p-value of 0.53. 


**Using the ‘final’ model chosen in part 4, focus on MSE to test the model predictive ability**
**a) Use a 10-fold cross-validation**

```{r}

kfold_cv = lapply(1:10, function(i){
  # create 10-fold training datasets
  data_train <- trainControl(method = "cv", number = 10)

  # Fit the model used above
  model_caret <- train((life_exp ~ murder + hs_grad + log_pop + frost),
                     data = states_analysis,
                     trControl = data_train,
                     method = 'lm',
                     na.action = na.pass)
  
  #return(list(model_caret$results, model_caret$resample))
  return(model_caret$results)
})

do.call("rbind", kfold_cv) %>%
  dplyr::select(-intercept) # %>% summarise(m = mean(RMSE))

```

**(b) Experiment a new, but simple bootstrap technique called “residual sampling”. Summarize the MSE.**

```{r}
set.seed(1)

# Perform a regression model with the original sample; calculate predicted values and residuals.
states_analysis = states_analysis %>%
  modelr::add_predictions(life_exp_fit) %>% # add predicted birthweight
  modelr::add_residuals(life_exp_fit) %>% # residual of observed bwt - predicted bwt 
  rename('pred1' = pred)


# function to bootstrap residuals and regress new predictions
boot.res <- function(data, index){
  data = data %>%
    rowwise %>%
    mutate(rand_res = sample(resid, replace = T, size = 1), # Randomly resample the residuals (with replacement), but leave the X values and predicted values unchanged.
           boot_y = pred1 + rand_res) %>% # New observations by adding the original predicted values to the bootstrap residuals
    modelr::add_predictions(lm(boot_y ~ murder + hs_grad + log_pop + frost, data = .)) %>%
    mutate(sq = (boot_y - pred)^2)

  #  new_pred = lm(boot_y ~ murder + hs_grad + log_pop + frost, data = data, subset = index) # regress new obs
  # return(new_pred)
  mse = 1/(nrow(data) - 4 - 1) * sum(data$sq) # calculate mse
  return(mse)
}

boot::boot(states_analysis, boot.res, 10)
boot::boot(states_analysis, boot.res, 1000)

```

The MSE of the bootstrap method is notably smaller than that of the 10-fold cross validation. This is intuitive on a smaller dataset such as the one used here (n = 50); a 10-fold cross validation using 50 data points trains the model on subsets of ~45, whereas the bootstrapping method uses sampled sets of size 50. It may be that at this sample size, this size difference of ~5 results in the bootstrap method having less MSE. This can be verified if we perform a LOOCV process:

```{r, warning=F}
loocv = lapply(1:10, function(i){
  # create 10-fold training datasets
  data_train <- trainControl(method = "cv", number = 50)

  # Fit the model used above
  model_caret <- train((life_exp ~ murder + hs_grad + log_pop + frost),
                     data = states_analysis,
                     trControl = data_train,
                     method = 'lm',
                     na.action = na.pass)
  
  #return(list(model_caret$results, model_caret$resample))
  return(model_caret$results)
})

do.call("rbind", loocv) %>%
  dplyr::select(-intercept) %>% 
  summarise(mse = mean(RMSE))

```

Increasing the size of the training subsets in the k-fold validation improves the MSE, though the bootstrap still performs better. Because of the senstivity of these two methods of CV, I would recommend running both to assess model performance. Both suggest that the model we selected above is subject to a fair bit of error, so we would want to revisit the model-building process. Ideally, we want a model whose performance does not vary notably by CV method AND has a small MSE. However, I think that the size of the dataset use here may unavoidably result in sensitive CV performance. A improved model will be one that reduces the MSE across both methods of CV used here. 
